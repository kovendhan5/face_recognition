{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000001755EA25270>, array([[[ 22,  21,  25],\n        [ 22,  21,  25],\n        [ 20,  19,  23],\n        ...,\n        [164, 164, 160],\n        [164, 164, 160],\n        [164, 164, 160]],\n\n       [[ 22,  21,  25],\n        [ 22,  21,  25],\n        [ 20,  19,  23],\n        ...,\n        [164, 164, 160],\n        [164, 164, 160],\n        [164, 164, 160]],\n\n       [[ 21,  20,  24],\n        [ 24,  23,  27],\n        [ 22,  21,  25],\n        ...,\n        [166, 166, 162],\n        [164, 164, 160],\n        [164, 164, 160]],\n\n       ...,\n\n       [[151, 154, 157],\n        [148, 151, 154],\n        [154, 157, 160],\n        ...,\n        [ 16,  16,  16],\n        [ 22,  22,  22],\n        [ 24,  24,  24]],\n\n       [[156, 159, 162],\n        [142, 145, 148],\n        [153, 156, 159],\n        ...,\n        [ 25,  19,  21],\n        [ 27,  19,  22],\n        [ 24,  26,  26]],\n\n       [[153, 156, 159],\n        [145, 148, 151],\n        [147, 150, 153],\n        ...,\n        [ 25,  24,  24],\n        [ 14,  25,  22],\n        [  6,  36,  26]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x000001757D0679B0>, 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Run the system with IP camera URL\u001b[39;00m\n\u001b[0;32m     95\u001b[0m ip_camera_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://192.0.0.4:8080/video\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your IP camera URL\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[43mrun_attendance_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mip_camera_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 55\u001b[0m, in \u001b[0;36mrun_attendance_system\u001b[1;34m(ip_camera_url)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Find all faces and face encodings in the frame\u001b[39;00m\n\u001b[0;32m     54\u001b[0m face_locations \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mface_locations(rgb_small_frame)\n\u001b[1;32m---> 55\u001b[0m face_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_small_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_locations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m face_encoding, face_location \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(face_encodings, face_locations):\n\u001b[0;32m     58\u001b[0m     matches \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mcompare_faces(known_encodings, face_encoding)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[38;5;241m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(\u001b[43mface_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_face_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_landmark_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_jitters\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000001755EA25270>, array([[[ 22,  21,  25],\n        [ 22,  21,  25],\n        [ 20,  19,  23],\n        ...,\n        [164, 164, 160],\n        [164, 164, 160],\n        [164, 164, 160]],\n\n       [[ 22,  21,  25],\n        [ 22,  21,  25],\n        [ 20,  19,  23],\n        ...,\n        [164, 164, 160],\n        [164, 164, 160],\n        [164, 164, 160]],\n\n       [[ 21,  20,  24],\n        [ 24,  23,  27],\n        [ 22,  21,  25],\n        ...,\n        [166, 166, 162],\n        [164, 164, 160],\n        [164, 164, 160]],\n\n       ...,\n\n       [[151, 154, 157],\n        [148, 151, 154],\n        [154, 157, 160],\n        ...,\n        [ 16,  16,  16],\n        [ 22,  22,  22],\n        [ 24,  24,  24]],\n\n       [[156, 159, 162],\n        [142, 145, 148],\n        [153, 156, 159],\n        ...,\n        [ 25,  19,  21],\n        [ 27,  19,  22],\n        [ 24,  26,  26]],\n\n       [[153, 156, 159],\n        [145, 148, 151],\n        [147, 150, 153],\n        ...,\n        [ 25,  24,  24],\n        [ 14,  25,  22],\n        [  6,  36,  26]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x000001757D0679B0>, 1"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load reference images and encode faces\n",
    "def load_student_encodings(image_folder=\"K:\\\\hackathon images\"):\n",
    "    student_names = []\n",
    "    encodings = []\n",
    "    \n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "            image = face_recognition.load_image_file(image_path)\n",
    "            image_encoding = face_recognition.face_encodings(image)[0]\n",
    "            encodings.append(image_encoding)\n",
    "            student_names.append(os.path.splitext(filename)[0])\n",
    "    \n",
    "    return student_names, encodings\n",
    "\n",
    "# Mark attendance in CSV\n",
    "def mark_attendance(name):\n",
    "    with open('attendance.csv', 'r+') as file:\n",
    "        existing_data = file.readlines()\n",
    "        attendance_list = [line.split(',')[0] for line in existing_data]\n",
    "        \n",
    "        if name not in attendance_list:\n",
    "            now = datetime.now()\n",
    "            time_string = now.strftime('%H:%M:%S')\n",
    "            date_string = now.strftime('%Y-%m-%d')\n",
    "            file.writelines(f'{name},{date_string},{time_string}\\n')\n",
    "\n",
    "# Main function to run facial recognition and mark attendance\n",
    "def run_attendance_system(ip_camera_url):\n",
    "    # Load student data\n",
    "    student_names, known_encodings = load_student_encodings()\n",
    "    \n",
    "    # Initialize IP webcam stream\n",
    "    video_capture = cv2.VideoCapture(ip_camera_url)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame. Check the IP camera stream.\")\n",
    "            break\n",
    "\n",
    "        # Resize frame for faster processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "        # Find all faces and face encodings in the frame\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        for face_encoding, face_location in zip(face_encodings, face_locations):\n",
    "            matches = face_recognition.compare_faces(known_encodings, face_encoding)\n",
    "            face_distances = face_recognition.face_distance(known_encodings, face_encoding)\n",
    "            \n",
    "            best_match_index = np.argmin(face_distances)\n",
    "            if matches[best_match_index]:\n",
    "                name = student_names[best_match_index]\n",
    "                mark_attendance(name)\n",
    "                \n",
    "                # Draw a box around the face\n",
    "                top, right, bottom, left = face_location\n",
    "                top *= 4\n",
    "                right *= 4\n",
    "                bottom *= 4\n",
    "                left *= 4\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "                \n",
    "                # Label the face\n",
    "                cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 255, 0), cv2.FILLED)\n",
    "                cv2.putText(frame, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Attendance System', frame)\n",
    "\n",
    "        # Break loop with 'q' key\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Create attendance file if it doesnâ€™t exist\n",
    "if not os.path.isfile('attendance.csv'):\n",
    "    with open('attendance.csv', 'w') as file:\n",
    "        file.write('Name,Date,Time\\n')\n",
    "\n",
    "# Run the system with IP camera URL\n",
    "ip_camera_url = \"http://192.0.0.4:8080/video\"  # Replace with your IP camera URL\n",
    "run_attendance_system(ip_camera_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000001FDCCCD1630>, array([[[129, 129, 129],\n        [132, 132, 132],\n        [134, 134, 134],\n        ...,\n        [127, 134, 136],\n        [117, 124, 126],\n        [124, 131, 133]],\n\n       [[132, 132, 132],\n        [135, 135, 135],\n        [134, 134, 134],\n        ...,\n        [133, 140, 142],\n        [135, 142, 144],\n        [112, 119, 121]],\n\n       [[134, 134, 134],\n        [134, 134, 134],\n        [137, 137, 137],\n        ...,\n        [130, 137, 139],\n        [129, 136, 138],\n        [138, 145, 147]],\n\n       ...,\n\n       [[ 99,  99,  99],\n        [102, 102, 102],\n        [100, 100, 100],\n        ...,\n        [ 77,  50,  37],\n        [ 75,  52,  38],\n        [ 62,  54,  35]],\n\n       [[100, 100, 100],\n        [100, 100, 100],\n        [101, 101, 101],\n        ...,\n        [ 71,  56,  43],\n        [ 84,  45,  41],\n        [ 61,  57,  41]],\n\n       [[100, 100, 100],\n        [100, 100, 100],\n        [101, 101, 101],\n        ...,\n        [ 65,  55,  41],\n        [ 75,  54,  44],\n        [ 45,  69,  44]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x000001FDAE369D30>, 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName,Date,Time\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m ip_camera_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://192.0.0.4:8080/video\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your IP camera URL\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[43mrun_attendance_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43mip_camera_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 49\u001b[0m, in \u001b[0;36mrun_attendance_system\u001b[1;34m(ip_camera_url)\u001b[0m\n\u001b[0;32m     46\u001b[0m face_locations \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mface_locations(rgb_small_frame)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m face_locations:  \u001b[38;5;66;03m# Check if any faces are detected\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     face_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_small_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_locations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m face_encoding, face_location \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(face_encodings, face_locations):\n\u001b[0;32m     52\u001b[0m         matches \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mcompare_faces(known_encodings, face_encoding)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\face_recognition\\api.py:214\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03mGiven an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m:return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m raw_landmarks \u001b[38;5;241m=\u001b[39m _raw_face_landmarks(face_image, known_face_locations, model)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(\u001b[43mface_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_face_descriptor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_landmark_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_jitters\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_face_descriptor(): incompatible function arguments. The following argument types are supported:\n    1. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], face: _dlib_pybind11.full_object_detection, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vector\n    2. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], num_jitters: int = 0) -> _dlib_pybind11.vector\n    3. (self: _dlib_pybind11.face_recognition_model_v1, img: numpy.ndarray[(rows,cols,3),numpy.uint8], faces: _dlib_pybind11.full_object_detections, num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectors\n    4. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], batch_faces: list[_dlib_pybind11.full_object_detections], num_jitters: int = 0, padding: float = 0.25) -> _dlib_pybind11.vectorss\n    5. (self: _dlib_pybind11.face_recognition_model_v1, batch_img: list[numpy.ndarray[(rows,cols,3),numpy.uint8]], num_jitters: int = 0) -> _dlib_pybind11.vectors\n\nInvoked with: <_dlib_pybind11.face_recognition_model_v1 object at 0x000001FDCCCD1630>, array([[[129, 129, 129],\n        [132, 132, 132],\n        [134, 134, 134],\n        ...,\n        [127, 134, 136],\n        [117, 124, 126],\n        [124, 131, 133]],\n\n       [[132, 132, 132],\n        [135, 135, 135],\n        [134, 134, 134],\n        ...,\n        [133, 140, 142],\n        [135, 142, 144],\n        [112, 119, 121]],\n\n       [[134, 134, 134],\n        [134, 134, 134],\n        [137, 137, 137],\n        ...,\n        [130, 137, 139],\n        [129, 136, 138],\n        [138, 145, 147]],\n\n       ...,\n\n       [[ 99,  99,  99],\n        [102, 102, 102],\n        [100, 100, 100],\n        ...,\n        [ 77,  50,  37],\n        [ 75,  52,  38],\n        [ 62,  54,  35]],\n\n       [[100, 100, 100],\n        [100, 100, 100],\n        [101, 101, 101],\n        ...,\n        [ 71,  56,  43],\n        [ 84,  45,  41],\n        [ 61,  57,  41]],\n\n       [[100, 100, 100],\n        [100, 100, 100],\n        [101, 101, 101],\n        ...,\n        [ 65,  55,  41],\n        [ 75,  54,  44],\n        [ 45,  69,  44]]], dtype=uint8), <_dlib_pybind11.full_object_detection object at 0x000001FDAE369D30>, 1"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def load_student_encodings(image_folder=r\"K:\\hackathon images\"):\n",
    "    student_names = []\n",
    "    encodings = []\n",
    "    \n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "            image = face_recognition.load_image_file(image_path)\n",
    "            image_encoding = face_recognition.face_encodings(image)\n",
    "            if image_encoding:  # Ensure encoding is not empty\n",
    "                encodings.append(image_encoding[0])\n",
    "                student_names.append(os.path.splitext(filename)[0])\n",
    "    \n",
    "    return student_names, encodings\n",
    "\n",
    "def mark_attendance(name):\n",
    "    with open('attendance.csv', 'r+') as file:\n",
    "        existing_data = file.readlines()\n",
    "        attendance_list = [line.split(',')[0] for line in existing_data]\n",
    "        \n",
    "        if name not in attendance_list:\n",
    "            now = datetime.now()\n",
    "            time_string = now.strftime('%H:%M:%S')\n",
    "            date_string = now.strftime('%Y-%m-%d')\n",
    "            file.writelines(f'{name},{date_string},{time_string}\\n')\n",
    "\n",
    "def run_attendance_system(ip_camera_url):\n",
    "    student_names, known_encodings = load_student_encodings()\n",
    "    video_capture = cv2.VideoCapture(ip_camera_url)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame. Check the IP camera stream.\")\n",
    "            break\n",
    "\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "\n",
    "        if face_locations:  # Check if any faces are detected\n",
    "            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "            \n",
    "            for face_encoding, face_location in zip(face_encodings, face_locations):\n",
    "                matches = face_recognition.compare_faces(known_encodings, face_encoding)\n",
    "                face_distances = face_recognition.face_distance(known_encodings, face_encoding)\n",
    "                \n",
    "                best_match_index = np.argmin(face_distances)\n",
    "                if matches[best_match_index]:\n",
    "                    name = student_names[best_match_index]\n",
    "                    mark_attendance(name)\n",
    "                    \n",
    "                    top, right, bottom, left = face_location\n",
    "                    top *= 4\n",
    "                    right *= 4\n",
    "                    bottom *= 4\n",
    "                    left *= 4\n",
    "                    cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "                    cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 255, 0), cv2.FILLED)\n",
    "                    cv2.putText(frame, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "        \n",
    "        cv2.imshow('Attendance System', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    video_capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if not os.path.isfile('attendance.csv'):\n",
    "    with open('attendance.csv', 'w') as file:\n",
    "        file.write('Name,Date,Time\\n')\n",
    "\n",
    "ip_camera_url = \"http://192.0.0.4:8080/video\"  # Replace with your IP camera URL\n",
    "run_attendance_system(ip_camera_url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
